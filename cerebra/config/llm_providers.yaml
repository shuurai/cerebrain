# LLM provider templates (defaults used at brain init).
# Override in brain state or ~/.cerebra/config.yaml per brain.
# Defaults: max_tokens=8192, temperature=0.7

defaults:
  max_tokens: 8192
  temperature: 0.7

providers:
  openrouter:
    model: "minimax/minimax-m2"
    api_base: "https://openrouter.ai/api/v1"
    max_tokens: 8192
    temperature: 0.7

  openai:
    model: "gpt-3.5-turbo"
    api_base: "https://api.openai.com/v1"
    max_tokens: 8192
    temperature: 0.7

  anthropic:
    model: "claude-3-5-sonnet-20241022"
    api_base: "https://api.anthropic.com"
    max_tokens: 8192
    temperature: 0.7

  ollama:
    model: "llama3"
    api_base: "http://localhost:11434"
    max_tokens: 8192
    temperature: 0.7

  local:
    model: "local"
    api_base: "http://localhost:5000"
    max_tokens: 8192
    temperature: 0.7
